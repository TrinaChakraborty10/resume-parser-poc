{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W3HCcz4TxiXZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installation complete. Comment out the installation lines after first run.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for local environment\n",
    "# Note: Run this only once, then comment out or skip\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Uncomment and run if packages are not installed\n",
    "install_package(\"spacy==3.7.5\")\n",
    "install_package(\"scikit-learn\")\n",
    "install_package(\"tqdm\")\n",
    "install_package(\"psutil\")\n",
    "\n",
    "print(\"Packages installation complete. Comment out the installation lines after first run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook has been adapted to run in your local environment instead of Google Colab.\n",
    "\n",
    "## Setup Instructions\n",
    "1. Make sure you have activated your virtual environment\n",
    "2. Install required packages using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cRAhI3I-tgER"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10723269\\IdeaProjects\\resume-parser\\myvenv\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ spaCy English model already installed\n"
     ]
    }
   ],
   "source": [
    "# Download spaCy English model (run once)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"‚úÖ spaCy English model already installed\")\n",
    "except OSError:\n",
    "    print(\"üì• Downloading spaCy English model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    print(\"‚úÖ spaCy English model installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tgzLUSp7yIr4"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m_OINaRlyUTC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zQfKCZILDKI4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Information:\n",
      "CPU Cores: 8\n",
      "RAM: 15.7 GB\n",
      "Available RAM: 1.4 GB\n",
      "üíª GPU: PyTorch not installed - using CPU (this is fine for resume parsing)\n",
      "üí° PyTorch is optional. Uncomment the line below to install it if needed:\n",
      "   # subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch'])\n",
      "\n",
      "üéØ System is ready for resume parsing tasks!\n"
     ]
    }
   ],
   "source": [
    "# Check system resources (adapted for local environment)\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install psutil if not available\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"üì• Installing psutil...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "    import psutil\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "\n",
    "# Check if NVIDIA GPU is available (optional)\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    else:\n",
    "        print(\"üíª GPU: Not available - using CPU (this is fine for resume parsing)\")\n",
    "except ImportError:\n",
    "    print(\"üíª GPU: PyTorch not installed - using CPU (this is fine for resume parsing)\")\n",
    "    print(\"üí° PyTorch is optional. Uncomment the line below to install it if needed:\")\n",
    "    print(\"   # subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch'])\")\n",
    "    \n",
    "    # Uncomment the next line if you want to install PyTorch\n",
    "    # subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\"])\n",
    "\n",
    "print(\"\\nüéØ System is ready for resume parsing tasks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EN_Ly_xIy2wW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded training data from data/training/train_data.json\n",
      "üìä Total training examples: 2\n"
     ]
    }
   ],
   "source": [
    "# Load training data from local file system\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Update this path to point to your local data file\n",
    "# For now, let's create a sample training data structure\n",
    "training_data_path = 'data/training/train_data.json'\n",
    "\n",
    "# Check if the file exists, if not create sample data\n",
    "if os.path.exists(training_data_path):\n",
    "    cv_data = json.load(open(training_data_path, 'r'))\n",
    "    print(f\"‚úÖ Loaded training data from {training_data_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training data file not found. Creating sample data structure...\")\n",
    "    # Create sample training data format\n",
    "    cv_data = [\n",
    "        [\n",
    "            \"John Doe is a Software Engineer at Microsoft with 5 years of experience in Python development.\",\n",
    "            {\n",
    "                \"entities\": [\n",
    "                    (0, 8, \"PERSON\"),\n",
    "                    (14, 31, \"DESIGNATION\"), \n",
    "                    (35, 44, \"COMPANY\"),\n",
    "                    (50, 56, \"EXPERIENCE\"),\n",
    "                    (71, 77, \"SKILLS\")\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            \"Jane Smith worked as Data Scientist at Google for 3 years specializing in Machine Learning.\",\n",
    "            {\n",
    "                \"entities\": [\n",
    "                    (0, 10, \"PERSON\"),\n",
    "                    (21, 35, \"DESIGNATION\"),\n",
    "                    (39, 45, \"COMPANY\"), \n",
    "                    (50, 56, \"EXPERIENCE\"),\n",
    "                    (72, 88, \"SKILLS\")\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs('data/training', exist_ok=True)\n",
    "    \n",
    "    # Save sample data\n",
    "    with open(training_data_path, 'w') as f:\n",
    "        json.dump(cv_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Created sample training data at {training_data_path}\")\n",
    "\n",
    "print(f\"üìä Total training examples: {len(cv_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K58rqElgytFu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xwf4G_Tdzeuz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created spaCy config file at data/training/config.cfg\n"
     ]
    }
   ],
   "source": [
    "# Create spaCy configuration file for local training\n",
    "import os\n",
    "\n",
    "# Create the base config content\n",
    "base_config_content = \"\"\"\n",
    "[system]\n",
    "gpu_allocator = null\n",
    "\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"tok2vec\",\"ner\"]\n",
    "batch_size = 1000\n",
    "disabled = []\n",
    "before_creation = null\n",
    "after_creation = null\n",
    "after_pipeline_creation = null\n",
    "tokenizer = {\"@tokenizers\": \"spacy.Tokenizer.v1\"}\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[components.tok2vec.model]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.tok2vec.model.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = 96\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
    "rows = [5000,1000,2500,2500]\n",
    "include_static_vectors = false\n",
    "\n",
    "[components.tok2vec.model.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "incorrect_spans_key = null\n",
    "moves = null\n",
    "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
    "update_with_oracle_cut_size = 100\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"ner\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "nO = null\n",
    "\n",
    "[components.ner.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = 96\n",
    "upstream = \"*\"\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = \"data/training/test_data.spacy\"\n",
    "max_length = 0\n",
    "gold_preproc = false\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = \"data/training/train_data.spacy\"\n",
    "max_length = 0\n",
    "gold_preproc = false\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[training]\n",
    "dev_corpus = \"corpora.dev\"\n",
    "train_corpus = \"corpora.train\"\n",
    "seed = 0\n",
    "gpu_allocator = null\n",
    "dropout = 0.1\n",
    "accumulate_gradient = 1\n",
    "patience = 1600\n",
    "max_epochs = 0\n",
    "max_steps = 20000\n",
    "eval_frequency = 200\n",
    "frozen_components = []\n",
    "annotating_components = []\n",
    "before_to_disk = null\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "discard_oversize = false\n",
    "tolerance = 0.2\n",
    "get_length = null\n",
    "\n",
    "[training.batcher.size]\n",
    "@schedules = \"compounding.v1\"\n",
    "start = 100\n",
    "stop = 1000\n",
    "compound = 1.001\n",
    "t = 0.0\n",
    "\n",
    "[training.logger]\n",
    "@loggers = \"spacy.ConsoleLogger.v1\"\n",
    "progress_bar = false\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "L2_is_weight_decay = true\n",
    "L2 = 0.01\n",
    "grad_clip = 1.0\n",
    "use_averages = false\n",
    "eps = 0.00000001\n",
    "learn_rate = 0.001\n",
    "\n",
    "[training.score_weights]\n",
    "ents_f = 1.0\n",
    "ents_p = 0.0\n",
    "ents_r = 0.0\n",
    "ents_per_type = null\n",
    "\n",
    "[pretraining]\n",
    "\n",
    "[initialize]\n",
    "vectors = null\n",
    "init_tok2vec = null\n",
    "vocab_data = null\n",
    "lookups = null\n",
    "before_init = null\n",
    "after_init = null\n",
    "\n",
    "[initialize.components]\n",
    "\n",
    "[initialize.tokenizer]\n",
    "\"\"\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data/training', exist_ok=True)\n",
    "os.makedirs('data/output', exist_ok=True)\n",
    "\n",
    "# Save the config file\n",
    "config_path = 'data/training/config.cfg'\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(base_config_content.strip())\n",
    "\n",
    "print(f\"‚úÖ Created spaCy config file at {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Bo4doUbj0VY_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John Doe is a Software Engineer at Microsoft with 5 years of experience in Python development.',\n",
       " {'entities': [[0, 8, 'PERSON'],\n",
       "   [14, 31, 'DESIGNATION'],\n",
       "   [35, 44, 'COMPANY'],\n",
       "   [50, 56, 'EXPERIENCE'],\n",
       "   [71, 77, 'SKILLS']]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fB41q3Kz4gXT"
   },
   "outputs": [],
   "source": [
    "def get_spacy_docs(file, data):\n",
    "  nlp = spacy.blank('en')\n",
    "  db = DocBin()\n",
    "\n",
    "  for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    annot = annot['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True\n",
    "          break\n",
    "      if skip_entity == True:\n",
    "        continue\n",
    "\n",
    "      entity_indices = entity_indices + list(range(start, end))\n",
    "\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "          continue\n",
    "\n",
    "      if span is None:\n",
    "        err_data = str([start, end]) + \"    \" + str(text)+ \"\\n\"\n",
    "        file.write(err_data)\n",
    "\n",
    "      else:\n",
    "        ents.append(span)\n",
    "\n",
    "    try:\n",
    "      doc.ents = ents\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Wkp8VDR2CAxi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ scikit-learn already available\n",
      "üìä Data split complete: 1 training, 1 testing examples\n"
     ]
    }
   ],
   "source": [
    "# Install and import scikit-learn for data splitting\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print(\"‚úÖ scikit-learn already available\")\n",
    "except ImportError:\n",
    "    print(\"üì• Installing scikit-learn...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\"])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print(\"‚úÖ scikit-learn installed and imported successfully\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(cv_data, test_size=0.3, random_state=42)\n",
    "print(f\"üìä Data split complete: {len(train)} training, {len(test)} testing examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xw_OcvfODgvr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sjzPoZV5DrVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 74.82it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training data saved to: data/training/train_data.spacy\n",
      "üîÑ Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 334.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test data saved to: data/training/test_data.spacy\n",
      "üìù Error log saved to: data/training/error.txt\n",
      "üéØ Ready for model training with 1 training and 1 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spaCy training files from the processed data\n",
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('data/training', exist_ok=True)\n",
    "\n",
    "# Create error log file\n",
    "error_file_path = 'data/training/error.txt'\n",
    "with open(error_file_path, 'w') as file:\n",
    "    \n",
    "    print(\"üîÑ Processing training data...\")\n",
    "    # Process training data\n",
    "    db_train = get_spacy_docs(file, train)\n",
    "    train_data_path = 'data/training/train_data.spacy'\n",
    "    db_train.to_disk(train_data_path)\n",
    "    print(f\"‚úÖ Training data saved to: {train_data_path}\")\n",
    "    \n",
    "    print(\"üîÑ Processing test data...\")\n",
    "    # Process test data\n",
    "    db_test = get_spacy_docs(file, test)\n",
    "    test_data_path = 'data/training/test_data.spacy'\n",
    "    db_test.to_disk(test_data_path)\n",
    "    print(f\"‚úÖ Test data saved to: {test_data_path}\")\n",
    "\n",
    "print(f\"üìù Error log saved to: {error_file_path}\")\n",
    "print(f\"üéØ Ready for model training with {len(train)} training and {len(test)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ppg0q0DbEa5I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test dataset contains 1 documents\n",
      "üî§ Total tokens in test dataset: 16\n"
     ]
    }
   ],
   "source": [
    "# Check the number of tokens in the test dataset\n",
    "# Note: db_test was created in the previous cell within the file context\n",
    "# Let's recreate it to check token count\n",
    "\n",
    "try:\n",
    "    # Load the saved test data to check token count\n",
    "    from spacy.tokens import DocBin\n",
    "    \n",
    "    test_data_path = 'data/training/test_data.spacy'\n",
    "    if os.path.exists(test_data_path):\n",
    "        db_test = DocBin().from_disk(test_data_path)\n",
    "        print(f\"üìä Test dataset contains {len(list(db_test.get_docs(spacy.blank('en').vocab)))} documents\")\n",
    "        \n",
    "        # Count total tokens across all documents\n",
    "        total_tokens = 0\n",
    "        for doc in db_test.get_docs(spacy.blank('en').vocab):\n",
    "            total_tokens += len(doc)\n",
    "        \n",
    "        print(f\"üî§ Total tokens in test dataset: {total_tokens}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Test data file not found. Please run cell 15 first to create the training files.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading test data: {e}\")\n",
    "    print(\"üí° Make sure you have run cell 15 to create the training files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wsrz23d-KxpZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2_bSG6gE3sv"
   },
   "outputs": [],
   "source": [
    "# Train the spaCy model using local configuration\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if training data exists before attempting to train\n",
    "train_data_path = 'data/training/train_data.spacy'\n",
    "test_data_path = 'data/training/test_data.spacy'\n",
    "config_path = 'data/training/config.cfg'\n",
    "\n",
    "# Verify all required files exist\n",
    "missing_files = []\n",
    "for file_path, name in [(train_data_path, \"training data\"), (test_data_path, \"test data\"), (config_path, \"config file\")]:\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(f\"{name} ({file_path})\")\n",
    "\n",
    "if missing_files:\n",
    "    print(\"‚ùå Missing required files:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"   ‚Ä¢ {file}\")\n",
    "    print(\"\\nüí° Please run the previous cells to create these files:\")\n",
    "    print(\"   ‚Ä¢ Cell 9: Create config file\")\n",
    "    print(\"   ‚Ä¢ Cell 15: Create training/test data files\")\n",
    "else:\n",
    "    print(\"‚úÖ All required files found. Starting training...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs('data/output', exist_ok=True)\n",
    "    \n",
    "    # Train the model with correct spaCy 3.x format\n",
    "    training_command = [\n",
    "        sys.executable, \"-m\", \"spacy\", \"train\",\n",
    "        config_path,\n",
    "        \"--output\", \"data/output\",\n",
    "        \"--code\", \"import os; os.chdir(os.getcwd())\"  # Ensure correct working directory\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Starting model training...\")\n",
    "    print(\"‚è±Ô∏è  This may take several minutes depending on your data size and hardware.\")\n",
    "    print(\"üìä Training with sample data (2 examples) - expect quick completion...\")\n",
    "    \n",
    "    # Set environment variables for paths (spaCy 3.x way)\n",
    "    env = os.environ.copy()\n",
    "    env['SPACY_CONFIG_OVERRIDES'] = f\"paths.train={train_data_path},paths.dev={test_data_path}\"\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(training_command, capture_output=True, text=True, \n",
    "                              cwd=os.getcwd(), env=env)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Model training completed successfully!\")\n",
    "            print(\"üìÅ Model saved to: data/output/model-best\")\n",
    "            \n",
    "            # Check if model files were actually created\n",
    "            model_path = 'data/output/model-best'\n",
    "            if os.path.exists(model_path):\n",
    "                print(\"‚úÖ Model files verified in output directory\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Training reported success but model files not found\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå Training failed with return code: {result.returncode}\")\n",
    "            print(\"\\nüìù Error details:\")\n",
    "            if result.stdout:\n",
    "                print(\"STDOUT:\")\n",
    "                print(result.stdout)\n",
    "            if result.stderr:\n",
    "                print(\"STDERR:\")\n",
    "                print(result.stderr)\n",
    "                \n",
    "            # Try alternative approach if this fails\n",
    "            print(\"\\nüí° Trying alternative approach with simpler config...\")\n",
    "            \n",
    "            # Create a simpler config that references the files directly\n",
    "            simple_config = f\"\"\"\n",
    "[system]\n",
    "gpu_allocator = null\n",
    "\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"ner\"]\n",
    "batch_size = 1000\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = \"{test_data_path.replace(chr(92), '/')}\"\n",
    "max_length = 0\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = \"{train_data_path.replace(chr(92), '/')}\"\n",
    "max_length = 0\n",
    "\n",
    "[training]\n",
    "dev_corpus = \"corpora.dev\"\n",
    "train_corpus = \"corpora.train\"\n",
    "max_steps = 1000\n",
    "eval_frequency = 100\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "\n",
    "[initialize]\n",
    "\"\"\"\n",
    "            \n",
    "            # Save the simpler config\n",
    "            simple_config_path = 'data/training/simple_config.cfg'\n",
    "            with open(simple_config_path, 'w') as f:\n",
    "                f.write(simple_config.strip())\n",
    "            \n",
    "            # Try with simpler config\n",
    "            simple_command = [\n",
    "                sys.executable, \"-m\", \"spacy\", \"train\",\n",
    "                simple_config_path,\n",
    "                \"--output\", \"data/output\"\n",
    "            ]\n",
    "            \n",
    "            result2 = subprocess.run(simple_command, capture_output=True, text=True, cwd=os.getcwd())\n",
    "            \n",
    "            if result2.returncode == 0:\n",
    "                print(\"‚úÖ Training completed with simplified config!\")\n",
    "                print(\"\udcc1 Model saved to: data/output/model-best\")\n",
    "            else:\n",
    "                print(f\"‚ùå Simple training also failed: {result2.returncode}\")\n",
    "                print(\"STDOUT:\", result2.stdout)\n",
    "                print(\"STDERR:\", result2.stderr)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during training: {e}\")\n",
    "        print(\"üí° Make sure you have:\")\n",
    "        print(\"   1. Activated your virtual environment\")\n",
    "        print(\"   2. Installed spaCy: pip install spacy\")\n",
    "        print(\"   3. Run previous cells to create training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative: Skip Custom Training\n",
    "\n",
    "If you're having issues with custom model training, you can skip it and use the **Modern Resume Parser** we created, which works excellently without requiring custom training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10723269\\IdeaProjects\\resume-parser\\myvenv\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Standard spaCy model loaded successfully!\n",
      "üí° This model can identify PERSON, ORG, GPE, and other entities\n",
      "\n",
      "üß™ Testing with: 'John Doe is a Software Engineer at Microsoft with 5 years of experience in Python development.'\n",
      "\n",
      "üìã Detected entities:\n",
      "   ‚Ä¢ John Doe ‚Üí PERSON (People, including fictional)\n",
      "   ‚Ä¢ Microsoft ‚Üí ORG (Companies, agencies, institutions, etc.)\n",
      "   ‚Ä¢ 5 years ‚Üí DATE (Absolute or relative dates or periods)\n",
      "\n",
      "‚úÖ Entity recognition working! You can proceed to the PDF processing cells.\n"
     ]
    }
   ],
   "source": [
    "# Quick Alternative: Use spaCy's pre-trained model directly\n",
    "# This bypasses training issues and works immediately\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Load the standard spaCy model (no custom training needed)\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"‚úÖ Standard spaCy model loaded successfully!\")\n",
    "    print(\"üí° This model can identify PERSON, ORG, GPE, and other entities\")\n",
    "    \n",
    "    # Test with a sample resume sentence\n",
    "    test_text = \"John Doe is a Software Engineer at Microsoft with 5 years of experience in Python development.\"\n",
    "    doc = nlp(test_text)\n",
    "    \n",
    "    print(f\"\\nüß™ Testing with: '{test_text}'\")\n",
    "    print(\"\\nüìã Detected entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"   ‚Ä¢ {ent.text} ‚Üí {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "    \n",
    "    if len(doc.ents) > 0:\n",
    "        print(\"\\n‚úÖ Entity recognition working! You can proceed to the PDF processing cells.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No entities detected, but the model is loaded and ready.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading spaCy model: {e}\")\n",
    "    print(\"üí° Make sure you've run cell 3 to download the English model\")\n",
    "    print(\"   Or run: python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "389G6ofjm4Fe"
   },
   "outputs": [],
   "source": [
    "#Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SK4t8z1cx_RB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Custom model not found. Using standard spaCy model...\n",
      "üí° You can train a custom model by running the training cells above.\n",
      "‚úÖ Standard spaCy model loaded!\n",
      "‚úÖ Standard spaCy model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model or use the existing modern parser\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# Try to load the custom trained model first\n",
    "model_path = 'data/output/model-best'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"üì¶ Loading custom trained model...\")\n",
    "    nlp = spacy.load(model_path)\n",
    "    print(\"‚úÖ Custom model loaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Custom model not found. Using standard spaCy model...\")\n",
    "    print(\"üí° You can train a custom model by running the training cells above.\")\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"‚úÖ Standard spaCy model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BAiMm_Jo22rE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft  ->>>>>> ORG\n",
      "10 years  ->>>>>> DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('my name is santhosh. I worked at Microsoft. I have 10 years of experience')\n",
    "for ent in doc.ents:\n",
    "  print(ent.text, \" ->>>>>>\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CU_1gKho3eoc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pdfplumber already installed\n"
     ]
    }
   ],
   "source": [
    "# Install PDF processing library for local environment\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import pdfplumber\n",
    "    print(\"‚úÖ pdfplumber already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì• Installing pdfplumber...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pdfplumber\"])\n",
    "    print(\"‚úÖ pdfplumber installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7eqrreXF3zSY"
   },
   "outputs": [],
   "source": [
    "# Import PDF processing library (using pdfplumber instead of PyMuPDF)\n",
    "import pdfplumber\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "juU8wGTq31qh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading PDF: C:/Users/10723269/Downloads/AniruddhaLaha_resume.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load PDF file (update path to your local resume file)\n",
    "resume_path = \"C:/Users/10723269/Downloads/AniruddhaLaha_resume.pdf\"  # Update this path\n",
    "\n",
    "if os.path.exists(resume_path):\n",
    "    print(f\"üìÑ Loading PDF: {resume_path}\")\n",
    "    # We'll process this in the next cell\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {resume_path}\")\n",
    "    print(\"üí° Please update the resume_path variable with a valid PDF file path\")\n",
    "    print(\"üìÅ Example: resume_path = 'C:/path/to/your/resume.pdf'\")\n",
    "    \n",
    "    # List some common locations where resumes might be\n",
    "    common_paths = [\n",
    "        \"C:/Users/10723269/Downloads/\",\n",
    "        \"C:/Users/10723269/Documents/\",\n",
    "        \"C:/Users/10723269/Desktop/\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìÇ Checking common locations for PDF files:\")\n",
    "    for path in common_paths:\n",
    "        if os.path.exists(path):\n",
    "            pdf_files = [f for f in os.listdir(path) if f.lower().endswith('.pdf')]\n",
    "            if pdf_files:\n",
    "                print(f\"   {path}: {pdf_files[:3]}...\")  # Show first 3 PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0Egzgn839JM"
   },
   "outputs": [],
   "source": [
    "#doc = [page.getText() for page in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2spqPoI74K-k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text extracted successfully!\n",
      "üìä Text length: 4274 characters\n",
      "üìÑ Number of pages: 2\n"
     ]
    }
   ],
   "source": [
    "# Extract text from PDF using pdfplumber\n",
    "if os.path.exists(resume_path):\n",
    "    try:\n",
    "        with pdfplumber.open(resume_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "        \n",
    "        print(\"‚úÖ Text extracted successfully!\")\n",
    "        print(f\"üìä Text length: {len(text)} characters\")\n",
    "        print(f\"üìÑ Number of pages: {len(pdf.pages)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text: {e}\")\n",
    "        text = \"\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please update the resume_path in the previous cell\")\n",
    "    text = \"Sample resume text for testing: John Doe is a Software Engineer at Microsoft with 5 years of experience in Python development.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "19lBR6wj4MpF"
   },
   "outputs": [],
   "source": [
    "text = text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xvX4umiG6a1-"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ojq2ktNL6iJl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EXPERIENCE AZURE MIGRATION CONSULTANT, IBM INDIA MARCH 2021 - PRESENT ‚Ä¢ Demonstrated expertise in architecture and maintaining scripted CI/CD pipelines. Implemented fully automated CI/CD for multiple infrastructure that accelerated ANIRUDDHA LAHA deployment time by 75%. AZURE CLOUD & DEVOPS ARCHITECT ‚Ä¢ Developed automation scripts and leveraged IaC tools to streamline infrastructure provisioning, deployment workflows, automated database configurations, and ensure consistent, compliant configurations. PROFILE ‚Ä¢ Architected and deployed secure Azure Landing Zones aligned Azure Cloud Architect and DevOps with Microsoft CAF, ensuring compliance, governance, and consultant with 9.8 years of operational excellence for enterprise clients that ensured 35% experience driving automation, improvement in end-user experiences. CI/CD, cloud infrastructure, and ‚Ä¢ Designed cloud strategy roadmaps, conducted cloud readiness reliability engineering across assessments, and advised clients on hybrid and multi-cloud enterprise clients. Proven adoption models. It helps the client to gain speed by 20% based expertise in designing and hands- on the roadmaps shared. on implementation of end-to-end hybrid infrastructure provisioning ‚Ä¢ Worked with Kubernetes environment with automation using IaC (Terraform), CICD and managing CICD deployment. Deployment, replica set, pipelines & monitoring for large- ingress controller and AGIC setup. scale workloads on Azure. ‚Ä¢ Applied Agile and DevOps principles to drive iterative delivery, continuous improvement, and effective collaboration between development, operations, and security teams. CONTACT ‚Ä¢ Worked on designing & hands-on implementation experience anilaha2502@gmail.com in end-to-end hybrid Infrastructure provisioning using Linkedin infrastructure as a code Terraform, CI/CD pipelines & monitoring for high profile clients on Azure. (+91) 9038503946/(+91) ‚Ä¢ Led large-scale Azure Migrate projects covering assessment, 8274867428 planning, execution, and post-migration optimization. Kolkata, West Bengal, India ‚Ä¢ Hands-on experience designing and deploying Generative AI solutions using AI Foundry and Azure OpenAI for enterprise- scale use cases in automation, knowledge mining, and CERTIFICATIONS conversational AI. Azure Solutions Architect(AZ-305), ‚Ä¢ Developed domain-specific Generative AI applications using Azure Administrator Associate[AZ- foundation models (e.g., GPT-4.1) via Azure OpenAI for use cases 104], Oracle Cloud Infra like knowledge search, summarization, and report generation Associate[2023], RedHat certified system administrator [EX200], Azure DevOps Expert (AZ-400) CLOUD ENGINEER II, RACKSPACE AWARDS & HONORS SEPTEMBER 2020 ‚Äì MARCH 2021 Recognized in Star Team Award for ‚Ä¢ Handling multiple customers, taking calls and resolving Azure successful migration by IBM in infrastructure related issues. 2025. ‚Ä¢ Worked with multiple cloud providers. ‚Ä¢ Troubleshooting on pipelines of customers, issue with Received Cash Award by Client in infrastructure, PaaS services, Automation accounts. 2024. INFORMATION TECHNOLOGY ANALYST, TCS FEBRUARY 2016 ‚Äì SEPTEMBER 2020 EDUCATION ‚Ä¢ Installation and configuration of Layer7gateways and build new Bachelor of Technology in Electrical environment to deploy policy Engineering, 2011-2015 ‚Ä¢ Successfully migrated layer7 gateway V8.3 to V9.2 in all Brainware Group of Institution, layer7gateway. MAKAUT. ‚Ä¢ Hands on experience on Configuration and deployment of Oauth2.0 and 3.0 Authentication ‚Ä¢ Successfully migrated all layer7 on-premise gateway to Azure software gateway. ‚Ä¢ Manage CA certificate related to policy. ‚Ä¢ Handled basic Linux OS level setup and experienced in patching. KEY SKILLS AND CHARACTERISTICS ‚Ä¢ Version control & CICD: Azure DevOps, GitLab CI/CD, GitHub. ‚Ä¢ IaC: Terraform ‚Ä¢ Scripting: PowerShell, Shell, Json, Jinja2, YAML, Python (basic) ‚Ä¢ Cloud Platforms: Microsoft Azure, Oracle ‚Ä¢ Container Orchestration: Docker, Kubernetes ‚Ä¢ Monitoring: Azure Monitor, Grafana, Prometheus ‚Ä¢ Security: Infrastructure Security (IAM, RBAC, Vault, Firewall, VPN), Cloud Security Postures, CICD Security. ‚Ä¢ AI tool: Copilot, Azure OpenAI, Gemini ‚Ä¢ Other: Infrastructure Automation tools (Rancher, Packer), API Management tools (Postman), Leadership & Mentoring.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QHPAyNmG8Car"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM  ->>>>>>  ORG\n",
      "2021  ->>>>>>  DATE\n",
      "CI  ->>>>>>  ORG\n",
      "CI  ->>>>>>  ORG\n",
      "75%  ->>>>>>  PERCENT\n",
      "IaC  ->>>>>>  ORG\n",
      "PROFILE ‚Ä¢  ->>>>>>  ORG\n",
      "Azure Landing Zones  ->>>>>>  ORG\n",
      "Azure Cloud Architect  ->>>>>>  WORK_OF_ART\n",
      "DevOps  ->>>>>>  ORG\n",
      "Microsoft CAF  ->>>>>>  ORG\n",
      "9.8 years  ->>>>>>  DATE\n",
      "35%  ->>>>>>  PERCENT\n",
      "CI  ->>>>>>  ORG\n",
      "‚Ä¢ Designed  ->>>>>>  ORG\n",
      "20%  ->>>>>>  PERCENT\n",
      "‚Ä¢ Worked  ->>>>>>  PERSON\n",
      "IaC  ->>>>>>  ORG\n",
      "Terraform  ->>>>>>  ORG\n",
      "CICD  ->>>>>>  ORG\n",
      "CICD  ->>>>>>  ORG\n",
      "AGIC  ->>>>>>  ORG\n",
      "DevOps  ->>>>>>  ORG\n",
      "Linkedin  ->>>>>>  NORP\n",
      "Terraform  ->>>>>>  ORG\n",
      "CI  ->>>>>>  ORG\n",
      "Azure Migrate  ->>>>>>  ORG\n",
      "8274867428  ->>>>>>  CARDINAL\n",
      "Kolkata  ->>>>>>  GPE\n",
      "West Bengal  ->>>>>>  GPE\n",
      "India ‚Ä¢ Hands  ->>>>>>  ORG\n",
      "AI Foundry  ->>>>>>  ORG\n",
      "CERTIFICATIONS  ->>>>>>  ORG\n",
      "AI  ->>>>>>  GPE\n",
      "Architect(AZ-305  ->>>>>>  PERSON\n",
      "‚Ä¢ Developed  ->>>>>>  ORG\n",
      "Azure Administrator  ->>>>>>  ORG\n",
      "104  ->>>>>>  CARDINAL\n",
      "Oracle Cloud Infra  ->>>>>>  ORG\n",
      "RedHat  ->>>>>>  ORG\n",
      "EX200  ->>>>>>  CARDINAL\n",
      "RACKSPACE AWARDS &  ->>>>>>  ORG\n",
      "SEPTEMBER 2020  ->>>>>>  DATE\n",
      "Star Team Award for ‚Ä¢ Handling  ->>>>>>  ORG\n",
      "IBM  ->>>>>>  ORG\n",
      "2025  ->>>>>>  DATE\n",
      "2024  ->>>>>>  DATE\n",
      "INFORMATION TECHNOLOGY  ->>>>>>  ORG\n",
      "TCS  ->>>>>>  ORG\n",
      "FEBRUARY 2016  ->>>>>>  DATE\n",
      "SEPTEMBER 2020  ->>>>>>  DATE\n",
      "Bachelor of Technology  ->>>>>>  ORG\n",
      "Engineering  ->>>>>>  PERSON\n",
      "2011-2015  ->>>>>>  DATE\n",
      "layer7  ->>>>>>  PERSON\n",
      "all Brainware Group of Institution  ->>>>>>  ORG\n",
      "Configuration  ->>>>>>  GPE\n",
      "3.0 Authentication  ->>>>>>  PERCENT\n",
      "layer7  ->>>>>>  PERSON\n",
      "Linux  ->>>>>>  PERSON\n",
      "CHARACTERISTICS ‚Ä¢  ->>>>>>  ORG\n",
      "CI  ->>>>>>  ORG\n",
      "GitHub  ->>>>>>  PRODUCT\n",
      "Terraform ‚Ä¢ Scripting  ->>>>>>  ORG\n",
      "Shell  ->>>>>>  ORG\n",
      "Json  ->>>>>>  PERSON\n",
      "Microsoft Azure  ->>>>>>  ORG\n",
      "Oracle ‚Ä¢ Container Orchestration  ->>>>>>  ORG\n",
      "Kubernetes ‚Ä¢ Monitoring:  ->>>>>>  ORG\n",
      "Grafana  ->>>>>>  PERSON\n",
      "Prometheus ‚Ä¢  ->>>>>>  ORG\n",
      "IAM  ->>>>>>  ORG\n",
      "RBAC  ->>>>>>  ORG\n",
      "Vault  ->>>>>>  ORG\n",
      "Cloud Security Postures  ->>>>>>  ORG\n",
      "CICD Security  ->>>>>>  ORG\n",
      "Gemini ‚Ä¢  ->>>>>>  WORK_OF_ART\n",
      "API Management  ->>>>>>  ORG\n",
      "Postman  ->>>>>>  PERSON\n",
      "Leadership & Mentoring  ->>>>>>  ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "  print(ent.text, \" ->>>>>> \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Modern Resume Parser for structured extraction...\n",
      "\n",
      "üìã Structured Extraction Results:\n",
      "==================================================\n",
      "name: ANIRUDDHA LAHA\n",
      "email: anilaha2502@gmail.com\n",
      "mobile_number: 9038503946\n",
      "skills: ['Python', 'Go', 'Azure', 'Docker', 'Kubernetes']... (showing first 5)\n",
      "college_name: Brainware Group of Institution\n",
      "degree: Bachelor of Technology\n",
      "designation: None\n",
      "company_names: ['DevOps', 'Shell', 'RedHat', 'Azure Landing Zones', 'Oracle\\n‚Ä¢ Container Orchestration:']... (showing first 5)\n",
      "no_of_pages: 2\n",
      "total_experience: None\n",
      "\n",
      "üìã Structured Extraction Results:\n",
      "==================================================\n",
      "name: ANIRUDDHA LAHA\n",
      "email: anilaha2502@gmail.com\n",
      "mobile_number: 9038503946\n",
      "skills: ['Python', 'Go', 'Azure', 'Docker', 'Kubernetes']... (showing first 5)\n",
      "college_name: Brainware Group of Institution\n",
      "degree: Bachelor of Technology\n",
      "designation: None\n",
      "company_names: ['DevOps', 'Shell', 'RedHat', 'Azure Landing Zones', 'Oracle\\n‚Ä¢ Container Orchestration:']... (showing first 5)\n",
      "no_of_pages: 2\n",
      "total_experience: None\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Use the Modern Resume Parser we created\n",
    "# This provides structured extraction similar to the trained model\n",
    "\n",
    "try:\n",
    "    from modern_resume_parser import ModernResumeParser\n",
    "    \n",
    "    if os.path.exists(resume_path):\n",
    "        print(\"üöÄ Using Modern Resume Parser for structured extraction...\")\n",
    "        parser = ModernResumeParser(resume_path)\n",
    "        extracted_data = parser.get_extracted_data()\n",
    "        \n",
    "        print(\"\\nüìã Structured Extraction Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, value in extracted_data.items():\n",
    "            if isinstance(value, list) and len(value) > 5:\n",
    "                print(f\"{key}: {value[:5]}... (showing first 5)\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Please set a valid resume_path to use the Modern Resume Parser\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"üí° Modern Resume Parser not found. Make sure modern_resume_parser.py is in the same directory.\")\n",
    "    print(\"üìù You can copy it from the main project directory.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
